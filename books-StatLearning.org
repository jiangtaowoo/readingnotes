#+LaTeX_CLASS: cn-article
#+TITLE: 统计学习方法笔记

* Chapter2 感知机
** 概念及定义
   + 感知机是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别
    （取+1, -1 二值）感知机对应于输入空间（特征空间）中将实例分为正负两类的
     分离超平面，属于判别模型
   + 损失函数最小化是关键
   + *定义 2.1* （感知机） \\
     假设输入空间（特征空间）是 \(X\subseteqR^{n}\) , 输出空间是
     y={+1,-1}. 输入 \(x\in X\) 表示实例的特征向量，对应输入空间（特征空间）的点
     输出 \(y\in Y\)表示实例的类别. 由输入空间到输出空间的如下函数 
     $$f(x)=sign(w \cdot x+b)               \eqno{(2.1)}$$
     称为感知机. 其中, w 和 b 为感知机模型参数,  \(w\in R^{n}\)叫做权值（weight）或
     权值向量（weight vector）， $b\in R$ 叫做偏置（bias）， $w \cdot x$  表示 w 和 x 的内积.
     sign 是符号函数, 即
     $$sign(x)=
     \begin{cases}
     \text{+1,   x}\geq0\\
     \text{-1,   x<0}
     \end{cases}$$
   + 使用: 感知机学习，由训练数据集（实例的特征向量及类别）
     $$ T=\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N}) \}$$
     其中， $x_{i}\in X\subseteqR^{n}$,  $y_{i}\in Y=\{+1,-1\}, i=1,2,...N$, 求得感知机
     模型(2.1)，即求得模型参数 w,b. 感知机预测，通过学习得到的感知机模型，对于新的
     输入实例给出其对应的输出类别
   + *定义 2.2* （数据集的线性可分性）\\
     存在超平面 $w \cdot x$ + b = 0, 则称数据集线性可分
   
** 感知机学习策略
*** 损失函数选择
     *Option1*: 误分类的点数总和，但这个损失函数对 w，b 不是连续可导，不易优化\\
     *Option2*: 误分类点到超平面 S 的总距离
     输入空间 $R^{n}$ 中任一点 $x_{0}$ 到超平面 S 的距离为
     $$\frac{1}{\|w\|}|w \cdot x_{0}+b|$$
     对于误分类的数据 $(x_{i},y_{i})$ 来说
     $$ -y_{i}(w \cdot x_{i}+b)>0$$
     误分类点 $x_{i}$ 到超平面 S 的距离是
     $$-\frac{1}{\|w\|}y_{i}(w \cdot x_{i}+b)$$
     所有误分类点到超平面 S 的总距离为
     $$-\frac{1}{\|w\|}\displaystyle\sum_{x_{i}\in M}y_{i}(w \cdot x_{i}+b)$$
     忽略 $\frac{1}{\|w\|}$, 定义感知机 sign( $w \cdot x$ +b)学习的损失函数为
     $$L(w,b)=-\displaystyle\sum_{x_{i}\in M}y_{i}(w \cdot x_{i}+b)      \eqno{(2.4)}$$
     其中 M 为误分类点的集合
     一个特定样本点的损失函数：在误分类时是参数 w,b 的线性函数，在正确分类时是 0
   
** 感知机学习算法
*** 感知机学习算法的原始形式
    求解
    $$\displaystyle min_{w,b}L(w,b)=- \displaystyle \sum_{x_{i}\in M}y_{i}(w \cdot x_{i}+b)$$
    其中 M 为误分类点的集合
    损失函数 L(w,b)的梯度由下面式子给出
    $$\nabla _{w}L(w,b)=- \displaystyle \sum_{x_{i}\in M}y_{i}x_{i}$$
    $$\nabla _{b}L(w,b)=- \displaystyle \sum_{x_{i}\in M}y_{i}$$
    随机选取误分类点 $(x_{i},y_{i})$ , 对 w,b 进行更新 $(\eta$ 为学习率, $0<\eta\leq 1)$ :
    $$ w \longleftarrow w + \eta y_{i}x_{i} $$
    $$ b \longleftarrow b + \eta y_{i} $$
    + *算法 2.1*\\
      输入: 线性可分数据集 T = $\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\}$,
      其中 $x_{i} \in X = R^{n}, y_{i} \in Y = \{-1,+1\}$, i=1,2,...,N; 学习率 $\eta (0<\eta\leq 1)$ ;\\
      输出: w,b; 感知机模型 f(x)=sign( $w \cdot x$ +b).
      1. 选取初值 $w_{0},b_{0}$
      2. 在训练集中选取数据 $(x_{i},y_{i})$
      3. 如果 $y_{i}(w \cdot x_{i}+b) \leq 0$
         $$ w \longleftarrow w + \eta y_{i}x_{i} $$
         $$ b \longleftarrow b + \eta y_{i} $$
      4. 转至 2, 直至训练集中没有误分类点
*** 感知机学习算法的对偶形式
    由误分类点 $(x_{i},y_{i})$ 的更新参数空间过程
         $$ w \longleftarrow w + \eta y_{i}x_{i} $$
         $$ b \longleftarrow b + \eta y_{i} $$
    可以看出, w,b 关于 $(x_{i},y_{i})$ 的增量分别是 $\alpha _{i}y_{i}x_{i}$ 和 $\alpha_{i}y_{i}$ \\
    这里 $\alpha _{i} = n_{i}\eta$ . 从学习过程，可以看出 w,b 可以分别表示为\\
         $$ w = \displaystyle \sum _{i=1}^{N} \alpha _{i}y_{i}x_{i}    \eqno{(2.14)}$$
         $$ b = \displaystyle \sum _{i=1}^{N} \alpha _{i}y_{i}    \eqno{(2.15)}$$
    + *算法 2.2* （感知机学习算法的对偶形式）\\
      输入: 线性可分数据集 T = $\{(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{N},y_{N})\}$,
      其中 $x_{i} \in X = R^{n}, y_{i} \in Y = \{-1,+1\}$, i=1,2,...,N; 学习率 $\eta (0<\eta\leq 1)$ ;\\
      输出: w,b; 感知机模型 f(x)=sign $\left( \displaystyle \sum _{j=1}^{N} \alpha _{j}y_{j}x_{j} \cdot x+b\right)$.
      1. 选取初值 $\alpha \leftarrow 0, b \leftarrow 0$
      2. 在训练集中选取数据 $(x_{i},y_{i})$
      3. 如果 $y_{i}\left(\displaystyle \sum _{j=1}^{N}\alpha _{j}y_{j}x_{j} \cdot x_{i}+b\right) \leq 0$
         $$ \alpha _{i} \longleftarrow \alpha _{i} + \eta $$
         $$ b \longleftarrow b + \eta y_{i} $$
      4. 转至 2, 直至训练集中没有误分类点
      对偶形式中训练实例仅以内积的形式出现. 为了方便, 可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储, 这个矩阵就是所谓的 Gram 矩阵。
      $$G = \left[ x_{i} \cdot x_{j} \right ]_{N \times N} $$
